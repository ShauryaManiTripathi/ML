{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slp implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x = (x - x.mean(axis=0)) / x.std(axis=0)\n",
    "#for k-fold\n",
    "n_folds = 5\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "epochs_list = [100, 200, 300]\n",
    "\n",
    "#store results\n",
    "all_metrics = []\n",
    "best_accuracy = 0\n",
    "best_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1 - sx)\n",
    "\n",
    "def custom_kfold(x, y, n_splits=5, shuffle=True, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    n_samples = len(x)\n",
    "    fold_size = n_samples // n_splits\n",
    "    indices = np.arange(n_samples)\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    folds = []\n",
    "    for i in range(n_splits):\n",
    "        start = i * fold_size\n",
    "        end = start + fold_size if i < n_splits - 1 else n_samples\n",
    "        val_indices = indices[start:end]\n",
    "        train_indices = np.concatenate([indices[:start], indices[end:]])\n",
    "        folds.append((train_indices, val_indices))\n",
    "    \n",
    "    return folds\n",
    "\n",
    "\n",
    "def one_hot_encode(y):\n",
    "    unique_classes = np.unique(y)\n",
    "    encoded = np.zeros((len(y), len(unique_classes)))\n",
    "    for i, label in enumerate(y):\n",
    "        encoded[i, label] = 1\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class singlelayerperceptron:\n",
    "    def __init__(self, input_size, num_classes, learning_rate=0.01, epochs=100):\n",
    "        self.weights = np.random.randn(input_size, num_classes) * 0.01\n",
    "        self.bias = np.zeros((1, num_classes))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.training_loss = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.z = np.dot(x, self.weights) + self.bias\n",
    "        return sigmoid(self.z)\n",
    "    \n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        return -np.mean(np.sum(y_true * np.log(np.clip(y_pred, 1e-10, 1.0)) + \n",
    "                              (1 - y_true) * np.log(np.clip(1 - y_pred, 1e-10, 1.0)), axis=1))\n",
    "    \n",
    "    def train(self, x, y, x_val=None, y_val=None):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        n_samples = x.shape[0]\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            y_pred = self.forward(x)\n",
    "            \n",
    "            error = y_pred - y\n",
    "            dw = np.dot(x.T, error * sigmoid_derivative(self.z)) / n_samples\n",
    "            db = np.mean(error * sigmoid_derivative(self.z), axis=0, keepdims=True)\n",
    "            \n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            train_loss = self.compute_loss(y_pred, y)\n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            if x_val is not None and y_val is not None:\n",
    "                val_pred = self.forward(x_val)\n",
    "                val_loss = self.compute_loss(val_pred, y_val)\n",
    "                val_losses.append(val_loss)\n",
    "        \n",
    "        return train_losses, val_losses\n",
    "    \n",
    "    def predict(self, x):\n",
    "        probabilities = self.forward(x)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    classes = np.unique(y_true)\n",
    "    metrics = {}\n",
    "    \n",
    "    for c in classes:\n",
    "        true_positive = np.sum((y_true == c) & (y_pred == c))\n",
    "        false_positive = np.sum((y_true != c) & (y_pred == c))\n",
    "        false_negative = np.sum((y_true == c) & (y_pred != c))\n",
    "        \n",
    "        precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "        recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "        \n",
    "        metrics[f'class_{c}'] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "    \n",
    "    overall_accuracy = np.mean(y_true == y_pred)\n",
    "    metrics['overall_accuracy'] = overall_accuracy\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "results for lr=0.01, epochs=100:\n",
      "average accuracy across folds: 0.8200\n",
      "class 0:\n",
      "  precision: 1.0000\n",
      "  recall: 1.0000\n",
      "class 1:\n",
      "  precision: 0.9750\n",
      "  recall: 0.4784\n",
      "class 2:\n",
      "  precision: 0.6497\n",
      "  recall: 0.9833\n",
      "\n",
      "results for lr=0.01, epochs=200:\n",
      "average accuracy across folds: 0.8067\n",
      "class 0:\n",
      "  precision: 1.0000\n",
      "  recall: 1.0000\n",
      "class 1:\n",
      "  precision: 0.9314\n",
      "  recall: 0.4602\n",
      "class 2:\n",
      "  precision: 0.6367\n",
      "  recall: 0.9667\n",
      "\n",
      "results for lr=0.01, epochs=300:\n",
      "average accuracy across folds: 0.8133\n",
      "class 0:\n",
      "  precision: 1.0000\n",
      "  recall: 1.0000\n",
      "class 1:\n",
      "  precision: 0.9314\n",
      "  recall: 0.4802\n",
      "class 2:\n",
      "  precision: 0.6469\n",
      "  recall: 0.9667\n",
      "\n",
      "results for lr=0.001, epochs=100:\n",
      "average accuracy across folds: 0.7800\n",
      "class 0:\n",
      "  precision: 0.9846\n",
      "  recall: 0.9714\n",
      "class 1:\n",
      "  precision: 0.6933\n",
      "  recall: 0.4685\n",
      "class 2:\n",
      "  precision: 0.6680\n",
      "  recall: 0.9000\n",
      "\n",
      "results for lr=0.001, epochs=200:\n",
      "average accuracy across folds: 0.7533\n",
      "class 0:\n",
      "  precision: 0.9846\n",
      "  recall: 1.0000\n",
      "class 1:\n",
      "  precision: 0.8000\n",
      "  recall: 0.2745\n",
      "class 2:\n",
      "  precision: 0.5817\n",
      "  recall: 0.9818\n",
      "\n",
      "results for lr=0.001, epochs=300:\n",
      "average accuracy across folds: 0.7733\n",
      "class 0:\n",
      "  precision: 0.9596\n",
      "  recall: 1.0000\n",
      "class 1:\n",
      "  precision: 0.8514\n",
      "  recall: 0.4358\n",
      "class 2:\n",
      "  precision: 0.6198\n",
      "  recall: 0.8667\n",
      "\n",
      "results for lr=0.0001, epochs=100:\n",
      "average accuracy across folds: 0.2067\n",
      "class 0:\n",
      "  precision: 0.1000\n",
      "  recall: 0.0923\n",
      "class 1:\n",
      "  precision: 0.1233\n",
      "  recall: 0.1527\n",
      "class 2:\n",
      "  precision: 0.3540\n",
      "  recall: 0.3845\n",
      "\n",
      "results for lr=0.0001, epochs=200:\n",
      "average accuracy across folds: 0.4933\n",
      "class 0:\n",
      "  precision: 0.5443\n",
      "  recall: 0.6000\n",
      "class 1:\n",
      "  precision: 0.3244\n",
      "  recall: 0.3366\n",
      "class 2:\n",
      "  precision: 0.4688\n",
      "  recall: 0.5856\n",
      "\n",
      "results for lr=0.0001, epochs=300:\n",
      "average accuracy across folds: 0.5933\n",
      "class 0:\n",
      "  precision: 0.7085\n",
      "  recall: 0.7762\n",
      "class 1:\n",
      "  precision: 0.6521\n",
      "  recall: 0.4576\n",
      "class 2:\n",
      "  precision: 0.3978\n",
      "  recall: 0.4952\n",
      "\n",
      "final results:\n",
      "best hyperparameters: learning rate = 0.01, epochs = 100\n",
      "best accuracy: 0.8667\n"
     ]
    }
   ],
   "source": [
    "folds = custom_kfold(x, y, n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for epochs in epochs_list:\n",
    "        fold_metrics = []\n",
    "        \n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(folds):\n",
    "            x_train, x_val = x[train_idx], x[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            y_train_encoded = one_hot_encode(y_train)\n",
    "            y_val_encoded = one_hot_encode(y_val)\n",
    "            \n",
    "            model = singlelayerperceptron(\n",
    "                input_size=x.shape[1],\n",
    "                num_classes=len(np.unique(y)),\n",
    "                learning_rate=lr,\n",
    "                epochs=epochs\n",
    "            )\n",
    "            \n",
    "            train_losses, val_losses = model.train(x_train, y_train_encoded, x_val, y_val_encoded)\n",
    "            \n",
    "            y_pred = model.predict(x_val)\n",
    "            \n",
    "            metrics = compute_metrics(y_val, y_pred)\n",
    "            metrics['learning_rate'] = lr\n",
    "            metrics['epochs'] = epochs\n",
    "            fold_metrics.append(metrics)\n",
    "            \n",
    "            if metrics['overall_accuracy'] > best_accuracy:\n",
    "                best_accuracy = metrics['overall_accuracy']\n",
    "                best_params = {'learning_rate': lr, 'epochs': epochs}\n",
    "        \n",
    "        avg_metrics = {\n",
    "            'learning_rate': lr,\n",
    "            'epochs': epochs,\n",
    "            'overall_accuracy': np.mean([m['overall_accuracy'] for m in fold_metrics])\n",
    "        }\n",
    "        \n",
    "        for class_idx in range(3):\n",
    "            avg_metrics[f'class_{class_idx}_precision'] = np.mean([m[f'class_{class_idx}']['precision'] for m in fold_metrics])\n",
    "            avg_metrics[f'class_{class_idx}_recall'] = np.mean([m[f'class_{class_idx}']['recall'] for m in fold_metrics])\n",
    "        \n",
    "        all_metrics.append(avg_metrics)\n",
    "        \n",
    "        print(f\"\\nresults for lr={lr}, epochs={epochs}:\")\n",
    "        print(f\"average accuracy across folds: {avg_metrics['overall_accuracy']:.4f}\")\n",
    "        for i in range(3):\n",
    "            print(f\"class {i}:\")\n",
    "            print(f\"  precision: {avg_metrics[f'class_{i}_precision']:.4f}\")\n",
    "            print(f\"  recall: {avg_metrics[f'class_{i}_recall']:.4f}\")\n",
    "\n",
    "print(\"\\nfinal results:\")\n",
    "print(f\"best hyperparameters: learning rate = {best_params['learning_rate']}, epochs = {best_params['epochs']}\")\n",
    "print(f\"best accuracy: {best_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
